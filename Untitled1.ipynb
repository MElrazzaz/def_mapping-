{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from pytorch_transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('aubmindlab/bert-base-arabert')\n",
    "model = BertModel.from_pretrained('aubmindlab/bert-base-arabert',output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform system check...\n",
      "check java version...\n",
      "Your java version is 1.8 which is compatiple with Farasa \n",
      "check toolkit binaries...\n",
      "Dependencies seem to be satisfied..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\me250041\\AppData\\Local\\Continuum\\anaconda2\\envs\\py3.7\\lib\\site-packages\\farasa\\__base.py:45: UserWarning: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "  \"Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing [SEGMENT] task in INTERACTIVE mode...\n",
      "task [SEGMENT] is initialized interactively.\n"
     ]
    }
   ],
   "source": [
    "from arabert.preprocess_arabert import never_split_tokens, preprocess\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "farasa_segmenter = FarasaSegmenter(interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"الجو جميل اليوم\"\n",
    "text_preprocessed = preprocess( text,\n",
    "                                do_farasa_tokenization = True,\n",
    "                                farasa = farasa_segmenter,\n",
    "                                use_farasapy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Here is the sentence I want embeddings for.\"\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text_preprocessed + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]        29,756\n",
      "ال              445\n",
      "+                 8\n",
      "جو              516\n",
      "جميل         15,724\n",
      "ال              445\n",
      "+                 8\n",
      "يوم           7,447\n",
      "[SEP]        29,758\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "# Put the model in \"evaluation\" mode,meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_contextualized_word_embedding(sentence,ID,)\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "# can use last hidden state as word embeddings\n",
    "    last_hidden_state = outputs[0]\n",
    "    word_embed_1 = last_hidden_state\n",
    "# Evaluating the model will return a different number of objects               based on how it's  configured in the `from_pretrained` call earlier. In this case, becase we set `output_hidden_states = True`, the third item will be the hidden states from all layers. See the documentation for more details:https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]\n",
    "# initial embeddings can be taken from 0th layer of hidden states\n",
    "    word_embed_2 = hidden_states[0]\n",
    "# sum of all hidden states\n",
    "    word_embed_3 = torch.stack(hidden_states).sum(0)\n",
    "# sum of second to last layer\n",
    "    word_embed_4 = torch.stack(hidden_states[2:]).sum(0)\n",
    "# sum of last four layer\n",
    "    word_embed_5 = torch.stack(hidden_states[-4:]).sum(0)\n",
    "# concatenate last four layers\n",
    "    word_embed_6 = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3072])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embed_6[:,1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())\n",
    "# Our final sentence embedding vector of shape: torch.Size([768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 7.0898e-01,  3.5331e-01,  5.3158e-01,  ..., -3.0989e-02,\n",
       "            6.6860e-01,  1.8452e-01],\n",
       "          [-9.9978e-02, -3.1447e-01,  5.7064e-01,  ..., -4.4019e-01,\n",
       "            2.4752e-01, -3.6739e-01],\n",
       "          [-3.8251e-01, -2.2150e-01,  2.5250e-01,  ..., -8.7480e-01,\n",
       "           -1.5606e-01, -2.7362e-01],\n",
       "          ...,\n",
       "          [ 3.1199e-01,  1.7777e+00,  1.5525e+00,  ..., -3.3180e-01,\n",
       "            7.2311e-01,  3.7617e-01],\n",
       "          [ 3.6201e-01,  1.4759e-02, -1.7863e-01,  ...,  1.5196e-03,\n",
       "            3.5863e-01, -4.1224e-01],\n",
       "          [-8.3098e-02, -1.7426e-01,  4.7426e-01,  ..., -2.6322e-02,\n",
       "            2.7918e-01, -1.0105e-01]]]),\n",
       " tensor([[ 9.9654e-01,  5.9111e-02, -3.9437e-02,  1.4281e-01,  1.1069e-01,\n",
       "          -2.2741e-01, -9.6528e-01, -2.2480e-01,  9.8895e-01, -3.4522e-01,\n",
       "           8.9446e-01,  1.8296e-01,  4.1006e-01, -9.5123e-01, -3.1468e-01,\n",
       "          -2.6477e-01, -2.6822e-02, -2.6737e-02,  9.9373e-01, -9.2041e-01,\n",
       "          -7.4245e-02, -8.0632e-02,  4.7173e-01,  3.6619e-01, -5.4777e-01,\n",
       "           9.8085e-01, -9.2622e-02, -2.4014e-01, -8.9283e-01,  7.3924e-02,\n",
       "           2.3433e-01, -5.7420e-02, -3.1496e-02, -9.8254e-01,  1.0545e-02,\n",
       "           2.0792e-01, -1.8635e-02, -3.7179e-02, -4.6875e-01,  2.0331e-01,\n",
       "           2.8340e-01, -1.5510e-01, -8.3324e-02,  8.5807e-01, -9.8748e-01,\n",
       "          -3.8719e-01, -4.5642e-01, -4.7265e-02, -3.6447e-01,  3.0921e-01,\n",
       "          -4.4141e-01,  1.0478e-01,  3.6060e-01,  6.2092e-02, -3.5803e-01,\n",
       "          -9.9023e-01,  1.0555e-01,  6.4170e-02, -3.1380e-01, -7.4083e-01,\n",
       "          -1.8992e-01, -7.3435e-01,  3.5220e-02, -8.0127e-04, -2.8971e-01,\n",
       "           8.5156e-01, -2.9127e-02,  4.1099e-02, -2.7882e-01,  2.6591e-01,\n",
       "           8.9229e-01, -9.9361e-01,  1.0750e-01, -5.7953e-02, -1.7917e-01,\n",
       "           1.9548e-01,  9.6133e-01,  9.0497e-01,  1.8470e-02,  1.1049e-01,\n",
       "           3.1038e-01, -9.7994e-01,  3.8219e-01,  7.9198e-01, -3.1239e-01,\n",
       "           3.5747e-01, -2.8193e-01, -1.9376e-01,  4.8031e-01, -9.8648e-01,\n",
       "          -9.5988e-01,  9.6331e-02,  4.6257e-01, -4.0335e-01, -7.3773e-01,\n",
       "           1.9355e-01,  3.4538e-01,  4.2059e-02,  9.1254e-01, -5.9072e-01,\n",
       "          -3.0366e-01, -7.4032e-02, -1.2696e-01,  8.0145e-01, -2.1687e-01,\n",
       "           6.3852e-01, -7.0181e-01, -8.2977e-03, -3.1073e-02,  6.1394e-01,\n",
       "          -1.4738e-01, -6.1318e-01, -9.2929e-01, -1.0063e-01, -8.4515e-01,\n",
       "           1.9244e-01,  9.7386e-01,  9.9559e-01,  5.5155e-02,  9.9206e-01,\n",
       "           5.4151e-01, -7.1449e-02, -9.2933e-02, -1.9993e-01, -8.0049e-02,\n",
       "          -3.0070e-01, -2.4008e-01,  9.0870e-02,  9.9139e-01, -2.2133e-01,\n",
       "           2.5732e-01,  8.1382e-01,  8.2773e-01,  2.6837e-01,  1.2223e-01,\n",
       "          -9.7983e-01, -7.8074e-01,  9.7026e-01,  1.8296e-01,  4.1162e-01,\n",
       "           4.3947e-01,  3.6109e-01, -3.7322e-01, -2.1666e-01,  9.6709e-01,\n",
       "          -9.8140e-01, -2.7806e-01, -9.1065e-01, -1.0452e-01,  9.9645e-01,\n",
       "           9.9757e-01,  9.7795e-01,  6.7702e-02,  3.5525e-01,  1.7804e-01,\n",
       "          -7.7781e-02,  9.5952e-01, -2.3274e-01,  9.1929e-01,  2.8678e-01,\n",
       "          -1.2805e-01, -1.5619e-01, -9.9408e-03, -5.7922e-01,  9.8677e-01,\n",
       "          -9.9489e-01,  3.8816e-01, -9.7894e-01,  3.9532e-02, -7.4234e-01,\n",
       "           1.0502e-01, -5.8416e-02, -8.4858e-02,  3.3112e-01, -3.8612e-02,\n",
       "          -1.5935e-01, -1.3722e-01,  2.1805e-01, -8.1672e-01, -4.0312e-02,\n",
       "          -1.3780e-01, -3.7417e-01,  6.7294e-01,  9.9066e-01,  1.6468e-01,\n",
       "           4.7273e-01,  3.5729e-01,  9.8837e-01,  4.1446e-01, -4.2931e-01,\n",
       "          -1.8531e-01,  4.7738e-02, -2.2002e-01,  8.1059e-01,  2.3848e-01,\n",
       "           7.7659e-02, -3.1852e-01, -9.3003e-01, -7.0150e-01,  1.6836e-01,\n",
       "           3.4869e-01, -9.2704e-01,  2.5485e-01, -6.4490e-01,  9.7618e-01,\n",
       "          -3.8151e-01,  4.4349e-01,  3.0639e-01, -2.3259e-01, -9.7040e-01,\n",
       "           4.1661e-01, -9.7931e-01, -9.9007e-01, -7.7205e-02,  2.0164e-01,\n",
       "          -2.5321e-01, -6.1224e-02, -1.0371e-01,  3.3929e-02, -2.8874e-01,\n",
       "           1.5928e-02,  3.5741e-01,  9.4785e-01, -4.6186e-01, -7.6925e-02,\n",
       "           6.9937e-02,  1.3542e-01,  3.5103e-01, -1.2865e-01, -3.1342e-01,\n",
       "          -2.4680e-01,  5.4865e-02,  3.7266e-02, -9.9296e-01,  2.3506e-01,\n",
       "          -9.9402e-01,  4.5893e-02, -2.3962e-01,  1.9463e-01,  9.3441e-01,\n",
       "          -4.0631e-01, -9.5175e-03,  9.5996e-01, -2.7650e-01,  1.9142e-01,\n",
       "          -3.0695e-01,  8.8076e-01,  9.2228e-01, -5.3806e-01,  6.5290e-02,\n",
       "          -7.4834e-01, -4.4644e-01, -3.4106e-01, -1.9902e-01,  3.2474e-01,\n",
       "          -2.8519e-01, -9.9485e-01,  9.0293e-01,  2.1727e-01,  1.4562e-01,\n",
       "           1.2565e-01, -5.2131e-01,  9.8592e-01, -5.5381e-01, -1.2037e-01,\n",
       "          -1.3740e-01,  8.8008e-01,  3.1722e-01, -9.8874e-02, -1.5521e-02,\n",
       "           1.1112e-01,  9.5554e-01, -2.9563e-02,  1.0363e-01, -3.3136e-01,\n",
       "          -1.9378e-01, -8.5846e-01,  3.3002e-01, -1.9400e-01, -8.7988e-01,\n",
       "           8.7602e-01,  9.9498e-01, -1.9457e-01, -1.5004e-01, -1.4408e-01,\n",
       "          -3.1359e-01,  8.8805e-02, -3.6254e-01, -1.6242e-01,  1.1292e-02,\n",
       "          -9.1452e-01,  5.4287e-01, -1.2087e-01, -9.9570e-01,  1.0018e-01,\n",
       "           1.0817e-01, -9.9734e-01, -1.9462e-01,  1.2373e-01, -5.9177e-01,\n",
       "          -3.0225e-01, -4.7684e-03,  3.4231e-01, -5.6739e-01, -1.0607e-01,\n",
       "          -8.7149e-03, -1.2055e-01, -3.8142e-01, -1.5622e-01,  1.6869e-01,\n",
       "           2.6075e-01,  8.7460e-01, -1.4272e-01, -7.1031e-01,  9.9177e-01,\n",
       "           6.2537e-02, -3.8119e-01, -1.0079e-02,  9.9459e-01, -2.6320e-02,\n",
       "           8.5076e-01, -2.5740e-02, -6.4829e-02,  1.7956e-01,  9.0824e-01,\n",
       "           9.8838e-02, -7.1861e-01, -1.1679e-01,  3.9747e-02,  9.9546e-01,\n",
       "          -1.8292e-01, -9.7829e-02, -6.7543e-01, -9.6367e-01,  9.5051e-01,\n",
       "           2.2925e-01, -5.2370e-01,  1.2475e-01,  1.5218e-01, -1.8759e-01,\n",
       "           2.4431e-01, -7.9835e-01,  2.3657e-01, -9.8492e-01, -1.3777e-01,\n",
       "           5.5496e-01,  4.1692e-01, -4.0468e-01, -5.8303e-01, -5.6187e-02,\n",
       "          -6.7329e-01,  4.2304e-02, -3.5396e-01,  3.9350e-01, -9.6716e-01,\n",
       "           9.7912e-01,  1.4400e-01,  4.0971e-01, -2.4604e-01, -2.6057e-01,\n",
       "          -9.6775e-01, -3.9244e-01, -3.0670e-01,  2.9621e-01,  9.9679e-01,\n",
       "           2.1008e-01, -9.7740e-01, -9.9686e-01, -2.3023e-02, -1.4476e-01,\n",
       "          -1.2873e-01, -8.5575e-01,  5.5077e-01, -7.1936e-01,  3.0558e-01,\n",
       "           9.9295e-02,  2.1482e-01, -6.4436e-02, -9.5931e-01,  2.6985e-01,\n",
       "          -6.5823e-01, -8.3555e-01, -2.6146e-01,  9.9260e-01,  9.2787e-01,\n",
       "           5.7498e-01,  3.6809e-02,  1.5292e-01, -1.5911e-01, -6.9725e-01,\n",
       "           9.8069e-01, -9.5778e-01, -6.5914e-01, -8.2617e-01, -4.4016e-01,\n",
       "           4.2702e-01, -2.0450e-01,  9.9500e-01, -8.4932e-02,  3.8212e-01,\n",
       "           3.1896e-01,  4.7168e-01,  9.9993e-01, -4.7660e-02, -9.4895e-02,\n",
       "           8.3445e-01, -5.0486e-01, -4.2307e-01,  1.8585e-01,  9.9556e-01,\n",
       "           1.4477e-01,  4.8387e-01, -1.2475e-02, -9.0951e-01,  9.6644e-01,\n",
       "          -1.3900e-01, -1.4408e-01,  7.7944e-02, -1.1037e-01,  2.3519e-01,\n",
       "          -2.9671e-01, -9.8626e-01, -1.2804e-01,  5.8170e-03, -2.7245e-02,\n",
       "           1.9657e-01,  2.5272e-01, -4.8082e-01,  1.5473e-01, -9.9329e-01,\n",
       "          -9.4305e-01,  9.9268e-01, -3.2475e-01, -1.5867e-01,  9.3229e-01,\n",
       "           9.8537e-01, -2.9261e-01, -9.0817e-01,  3.5690e-01,  9.7423e-01,\n",
       "           4.9962e-02,  7.7006e-01,  6.4456e-02,  7.1560e-01,  1.6280e-01,\n",
       "           2.2454e-02,  9.3802e-01,  2.7329e-01,  1.9070e-01, -4.2414e-01,\n",
       "           9.2145e-01,  1.6066e-01, -1.7039e-01,  2.2037e-01, -2.9635e-01,\n",
       "           9.9324e-01,  3.0487e-01,  2.7111e-01, -6.4633e-02,  5.3584e-02,\n",
       "           8.4226e-02,  3.7916e-01,  1.5325e-01, -2.3433e-01, -1.7640e-02,\n",
       "           4.6974e-01,  5.7583e-01,  2.3990e-01, -2.1130e-02, -9.4327e-01,\n",
       "          -1.8469e-01, -8.6133e-01,  7.9697e-02, -9.7869e-01, -2.6239e-01,\n",
       "           3.3767e-01, -2.0917e-01, -1.1622e-01, -2.3179e-01,  1.7261e-01,\n",
       "           9.8368e-01, -8.6476e-01, -2.1674e-03, -4.1659e-01,  8.4606e-01,\n",
       "          -3.6305e-01,  9.8216e-01, -1.1449e-01,  2.8534e-01, -4.9996e-02,\n",
       "           8.3372e-01, -8.8816e-01, -7.8970e-03,  2.1948e-01,  9.4636e-02,\n",
       "           8.5589e-01, -4.3584e-01,  6.1272e-01,  1.3691e-01,  2.2967e-01,\n",
       "           2.8540e-01,  3.6975e-01,  4.3415e-01,  9.1225e-02,  1.5978e-01,\n",
       "          -7.6249e-01,  5.6860e-01,  9.9905e-01, -2.4452e-01,  2.6759e-01,\n",
       "           4.5529e-01,  9.0857e-02, -1.3400e-01,  1.6306e-01,  1.1826e-02,\n",
       "           8.9837e-01,  8.6781e-01, -3.9072e-01,  9.9906e-01, -3.6731e-01,\n",
       "           9.6609e-01, -9.9925e-01,  5.0256e-02,  3.2119e-01, -9.4285e-01,\n",
       "           3.1405e-01, -4.0615e-01,  3.8568e-01,  1.2905e-01, -9.8082e-01,\n",
       "          -7.3639e-01, -9.6667e-02, -6.4248e-01,  2.3094e-01,  5.7916e-02,\n",
       "          -2.0284e-01,  1.6109e-01,  2.8525e-02, -8.4524e-01,  1.2810e-01,\n",
       "          -3.4450e-01,  1.3060e-01,  2.1248e-01,  3.6870e-01,  1.5019e-01,\n",
       "          -3.7662e-01,  1.1206e-01, -9.8250e-01, -1.1313e-01,  2.5828e-01,\n",
       "           3.7861e-01,  3.3476e-01, -2.1503e-02,  9.9362e-01,  8.4604e-01,\n",
       "           9.7602e-01,  9.9426e-01, -1.5248e-01,  9.8420e-01,  2.3898e-01,\n",
       "           7.2248e-01,  9.3397e-02, -8.1506e-02, -3.5248e-01, -3.2573e-01,\n",
       "          -4.0916e-01, -6.5965e-02,  9.0417e-01, -1.4129e-01, -2.4682e-01,\n",
       "           3.1656e-01,  2.3146e-02,  9.3656e-02, -2.1402e-01, -2.2880e-01,\n",
       "          -1.0927e-01,  4.1998e-02,  2.9781e-01, -8.9902e-03,  4.5645e-01,\n",
       "          -9.8045e-01,  1.3995e-01,  9.8730e-01, -5.7153e-01,  2.2191e-01,\n",
       "          -1.5351e-02,  8.8959e-02, -5.2299e-02,  2.5430e-01,  1.5494e-02,\n",
       "          -8.8018e-02, -8.1522e-03,  3.5429e-01,  1.4885e-01, -1.1155e-01,\n",
       "           6.7845e-02, -5.2836e-02, -5.8345e-02,  9.8454e-01,  1.7238e-01,\n",
       "           2.7696e-01,  2.2180e-01,  2.2393e-01, -7.6471e-03, -3.7256e-01,\n",
       "          -3.3701e-01, -7.1548e-02, -9.6398e-01, -5.4532e-01, -1.9996e-01,\n",
       "           8.4503e-01, -2.9901e-01,  9.1321e-02,  1.3015e-02,  1.2656e-01,\n",
       "          -8.6206e-01,  5.7261e-02, -2.0140e-02, -9.9062e-01,  7.6435e-02,\n",
       "           1.1077e-01,  1.8982e-01, -1.7664e-01,  4.1011e-01,  2.4611e-02,\n",
       "           8.2048e-01, -1.1721e-01,  1.3901e-01, -1.4791e-01, -3.4339e-01,\n",
       "          -2.4964e-01,  8.0538e-02, -7.3135e-01, -7.2388e-01,  8.8740e-01,\n",
       "           3.3870e-01,  2.9417e-01, -1.5858e-01, -9.9062e-01,  9.4992e-01,\n",
       "           2.4154e-01,  8.8189e-01,  7.6245e-01, -9.9188e-01, -9.2602e-01,\n",
       "           1.1522e-01,  5.0436e-02, -9.8761e-01, -9.7972e-01,  9.6935e-01,\n",
       "           1.0417e-01, -3.5409e-01, -9.6101e-01, -8.9986e-01,  1.8363e-01,\n",
       "           7.1486e-01, -6.4815e-01, -5.1370e-01, -3.8610e-01, -1.8066e-01,\n",
       "          -4.0075e-01,  1.7499e-01,  3.1998e-01,  4.4648e-02,  3.4953e-02,\n",
       "          -1.1410e-01, -9.8629e-01,  4.1599e-02,  3.1370e-01,  7.2328e-02,\n",
       "          -9.9898e-02, -2.4270e-01, -9.8299e-01,  3.8883e-01, -8.3896e-01,\n",
       "           1.0378e-01,  6.1686e-02,  2.4102e-02,  1.8697e-01,  3.6515e-01,\n",
       "          -8.9719e-01, -1.3708e-01,  3.2645e-01,  2.9152e-01, -8.6166e-02,\n",
       "           1.9098e-01,  3.1745e-02,  7.0461e-01, -6.9850e-01, -3.8629e-01,\n",
       "          -7.4318e-01, -7.7463e-01,  5.1432e-01, -9.6140e-01, -2.5807e-01,\n",
       "          -4.9861e-01,  8.4412e-01, -9.9185e-01,  8.6069e-01,  4.9099e-02,\n",
       "           9.9152e-01, -5.4918e-01, -9.9448e-01, -4.5909e-01,  1.7227e-01,\n",
       "           5.6273e-01, -2.3061e-01,  1.7841e-01, -1.4855e-03,  9.1724e-01,\n",
       "           6.6873e-02, -4.7653e-01, -9.9352e-01,  1.3855e-01, -6.4533e-01,\n",
       "          -3.8744e-01,  9.6605e-01,  8.3520e-01, -1.2496e-01,  3.0204e-01,\n",
       "          -5.5672e-01, -1.3496e-01, -2.3859e-02, -1.0600e-01,  9.9090e-01,\n",
       "          -4.9156e-01,  9.7015e-01,  1.2498e-01, -8.1530e-02, -4.4759e-02,\n",
       "          -2.9701e-02,  2.3576e-01, -9.6236e-02,  8.2214e-01, -6.5467e-01,\n",
       "           1.4191e-01, -9.5308e-01, -9.6716e-01,  9.7863e-01, -1.1200e-01,\n",
       "          -2.3688e-02,  6.3139e-02,  2.1468e-01, -4.2035e-01, -7.7958e-01,\n",
       "           2.3474e-01,  2.8102e-01,  5.0282e-02,  4.5235e-02, -1.4123e-01,\n",
       "           8.4088e-03, -4.0455e-01, -3.7986e-01, -8.6186e-02,  9.0351e-02,\n",
       "           2.9356e-02, -1.0302e-02, -2.3143e-01,  9.8363e-01, -4.1844e-01,\n",
       "           9.8928e-02,  8.6545e-02,  2.4543e-01, -9.1159e-02,  5.7121e-01,\n",
       "          -9.4626e-01, -2.5495e-01, -4.4196e-01]]),\n",
       " (tensor([[[ 0.5077, -0.2188, -0.5519,  ..., -0.1319,  0.0223,  0.2943],\n",
       "           [-0.1954, -0.7959,  0.5031,  ..., -0.6276, -1.0000,  0.3592],\n",
       "           [-0.3576, -0.4192, -0.0649,  ..., -0.0541, -0.1604,  0.4017],\n",
       "           ...,\n",
       "           [-0.6830, -0.0531, -0.3544,  ...,  0.0743, -0.1598,  0.4179],\n",
       "           [-0.1707, -0.3558, -0.4727,  ..., -0.1374, -0.4766, -0.7328],\n",
       "           [-0.8074, -0.2318, -0.3329,  ...,  0.7887, -0.8650, -0.1106]]]),\n",
       "  tensor([[[ 0.5604,  0.1270, -0.2997,  ..., -0.2214,  0.2805,  0.2673],\n",
       "           [-0.1622, -0.5777,  0.6308,  ..., -1.3057, -0.4816,  0.2652],\n",
       "           [-0.8080, -0.4010,  0.1098,  ..., -0.8941,  0.3147,  0.6521],\n",
       "           ...,\n",
       "           [-0.9645, -0.0239, -0.1186,  ..., -0.7022,  0.3539,  0.6065],\n",
       "           [ 0.0450, -0.2993, -0.8088,  ..., -0.8927, -0.5323, -0.5663],\n",
       "           [-0.6598,  0.1833, -0.0086,  ...,  0.1781, -0.5580,  0.1651]]]),\n",
       "  tensor([[[ 0.5258,  0.3588, -0.4253,  ..., -0.2139,  0.2052,  0.6456],\n",
       "           [ 0.3297,  0.0573,  0.7648,  ..., -0.9342, -1.0904,  0.2174],\n",
       "           [-0.3742, -0.2628, -0.0648,  ..., -0.7009, -0.0905,  0.3920],\n",
       "           ...,\n",
       "           [-0.4133,  0.0742, -0.2182,  ..., -0.4301, -0.1512,  0.4346],\n",
       "           [ 0.0461, -0.0146, -0.8251,  ..., -0.7339, -0.8886, -0.3419],\n",
       "           [-0.2305,  0.3751,  0.2032,  ..., -0.1991, -0.5546,  0.5815]]]),\n",
       "  tensor([[[ 0.3761, -0.0024, -0.1406,  ..., -0.5217,  0.1819,  0.3793],\n",
       "           [ 0.3295, -0.1985,  1.1434,  ..., -0.8030, -0.9025,  0.2500],\n",
       "           [ 0.2240, -0.0653,  0.2843,  ..., -0.7280, -0.0734,  0.3603],\n",
       "           ...,\n",
       "           [ 0.0438, -0.0272,  0.0714,  ..., -0.6576, -0.1590,  0.2726],\n",
       "           [ 0.5180, -0.3029, -0.7303,  ..., -0.4935, -0.6912, -0.5146],\n",
       "           [-0.0818,  0.1212,  0.2984,  ..., -0.4123, -0.3718,  0.4389]]]),\n",
       "  tensor([[[ 3.7863e-01,  7.8349e-02, -3.5298e-01,  ..., -4.4433e-01,\n",
       "            -8.6406e-04,  1.0658e-01],\n",
       "           [ 3.3176e-01, -3.6581e-01,  9.6780e-01,  ..., -1.5685e-01,\n",
       "            -8.3553e-01, -9.3664e-02],\n",
       "           [ 2.2094e-01, -1.6573e-01,  3.2132e-01,  ..., -4.4002e-01,\n",
       "             9.2676e-02,  2.9391e-01],\n",
       "           ...,\n",
       "           [-2.4649e-02, -2.6998e-01,  2.0632e-02,  ..., -3.8935e-01,\n",
       "            -1.3317e-01,  2.5366e-01],\n",
       "           [ 3.5928e-01, -3.1731e-01, -5.1361e-01,  ..., -2.1146e-01,\n",
       "            -7.0314e-01, -9.1718e-01],\n",
       "           [-6.9479e-02, -1.0781e-01,  1.7603e-01,  ...,  3.3817e-02,\n",
       "            -3.5789e-01, -3.3524e-02]]]),\n",
       "  tensor([[[ 5.8222e-01,  6.2120e-01, -3.4423e-01,  ..., -3.2553e-01,\n",
       "             2.4091e-02, -3.9118e-01],\n",
       "           [ 3.1653e-01, -1.9573e-01,  1.4098e+00,  ..., -8.7702e-02,\n",
       "            -5.4984e-01, -2.8913e-01],\n",
       "           [-1.2712e-01,  1.1939e-01,  2.6934e-01,  ..., -2.2760e-01,\n",
       "             4.2560e-01, -1.1619e-03],\n",
       "           ...,\n",
       "           [-3.2287e-01,  5.8419e-02, -1.9364e-01,  ..., -3.4266e-01,\n",
       "             1.5709e-01,  1.0140e-02],\n",
       "           [ 2.6097e-01, -1.8545e-01, -5.1545e-01,  ..., -2.6351e-01,\n",
       "            -4.1000e-01, -1.0903e+00],\n",
       "           [ 5.4949e-02,  1.9012e-01,  3.9114e-02,  ...,  2.6788e-01,\n",
       "            -4.6628e-01, -2.5936e-01]]]),\n",
       "  tensor([[[ 0.7097, -0.1116, -0.4392,  ..., -0.5297,  0.6921, -0.7277],\n",
       "           [ 0.5469, -0.4994,  1.3627,  ..., -0.0694, -0.1597, -0.4387],\n",
       "           [-0.2597,  0.1039, -0.1213,  ..., -0.4044,  0.6103, -0.5112],\n",
       "           ...,\n",
       "           [-0.1565,  0.0259, -0.5231,  ..., -0.5803,  0.4660, -0.2539],\n",
       "           [ 0.4408, -0.7095, -0.9069,  ..., -0.4785, -0.3076, -1.2176],\n",
       "           [ 0.0080,  0.0310, -0.1406,  ...,  0.1048, -0.0170, -0.3268]]]),\n",
       "  tensor([[[ 0.7261,  0.1990, -0.3879,  ..., -0.0574,  0.9200, -0.6860],\n",
       "           [ 0.3806,  0.0427,  1.0262,  ..., -0.2908, -0.2963, -0.7745],\n",
       "           [-0.4475,  0.2948,  0.3555,  ..., -0.6178,  0.7086, -0.3625],\n",
       "           ...,\n",
       "           [-0.3979,  0.1014, -0.3264,  ..., -0.9084,  0.6472, -0.1762],\n",
       "           [-0.4818, -0.2207, -1.2728,  ...,  0.1998,  0.1468, -1.0732],\n",
       "           [-0.1226,  0.2339, -0.1489,  ...,  0.3649, -0.0068, -0.4359]]]),\n",
       "  tensor([[[ 1.0047, -0.0716, -0.6680,  ..., -0.1256,  0.7109, -0.3899],\n",
       "           [-0.1068,  0.0061,  1.3505,  ..., -0.4763,  0.2472, -1.0952],\n",
       "           [-0.3594,  0.1083,  0.5493,  ..., -0.8076,  0.6722, -0.2298],\n",
       "           ...,\n",
       "           [-0.0768,  0.0367, -0.6536,  ..., -0.9401,  0.7094, -0.2086],\n",
       "           [ 0.0626, -0.0016, -1.2734,  ...,  0.1601,  0.3541, -1.1323],\n",
       "           [ 0.0596,  0.5454, -0.3530,  ...,  0.8400,  0.7271, -0.7394]]]),\n",
       "  tensor([[[ 1.1285, -0.0202, -0.4338,  ..., -0.3181,  1.1025, -0.3672],\n",
       "           [-0.2362, -0.3305,  1.3952,  ..., -0.3884,  0.7166, -0.9321],\n",
       "           [-0.0946, -0.1417,  0.2942,  ..., -1.1435, -0.0073,  0.4593],\n",
       "           ...,\n",
       "           [ 0.5588, -0.3312, -0.5022,  ..., -0.7766,  0.8606,  0.1651],\n",
       "           [ 0.3140,  0.0659, -1.1862,  ...,  0.3900,  0.5132, -0.7065],\n",
       "           [ 0.2854, -0.0402, -0.5248,  ...,  0.3594,  0.1900, -0.2625]]]),\n",
       "  tensor([[[ 9.0111e-01, -1.4452e-01, -3.4184e-01,  ..., -2.6663e-01,\n",
       "             1.1004e+00, -2.8361e-01],\n",
       "           [-2.2328e-01, -3.4722e-01,  1.1184e+00,  ..., -3.3104e-01,\n",
       "             5.0972e-01, -9.2450e-01],\n",
       "           [ 7.4774e-02, -4.6216e-01, -2.0427e-01,  ..., -1.2894e+00,\n",
       "             1.1508e-01, -1.3082e-02],\n",
       "           ...,\n",
       "           [ 1.5430e-01, -1.3373e-01, -2.0250e-01,  ..., -3.1949e-04,\n",
       "             2.0571e-01, -1.1533e-02],\n",
       "           [ 4.2552e-01, -5.0029e-01, -1.3034e+00,  ..., -2.3949e-01,\n",
       "             8.1520e-01, -1.1044e+00],\n",
       "           [ 1.7076e-01,  1.5143e-01, -1.0497e-01,  ...,  2.4731e-01,\n",
       "             5.2762e-01, -2.6084e-01]]]),\n",
       "  tensor([[[ 1.1198,  0.0285, -0.0764,  ...,  0.1814,  1.2220,  0.0911],\n",
       "           [-0.3531, -0.4926,  0.4824,  ..., -0.6033,  0.1910, -0.5747],\n",
       "           [-0.3549, -0.3465,  0.1328,  ..., -1.2126, -0.2281, -0.0871],\n",
       "           ...,\n",
       "           [ 0.0658, -0.0539, -0.0288,  ..., -0.0299,  0.1045,  0.1591],\n",
       "           [ 0.5177, -0.3554, -0.6704,  ..., -0.0761,  0.3923, -0.6201],\n",
       "           [ 0.0603, -0.2293,  0.2729,  ...,  0.2793,  0.6266, -0.1205]]]),\n",
       "  tensor([[[ 7.0898e-01,  3.5331e-01,  5.3158e-01,  ..., -3.0989e-02,\n",
       "             6.6860e-01,  1.8452e-01],\n",
       "           [-9.9978e-02, -3.1447e-01,  5.7064e-01,  ..., -4.4019e-01,\n",
       "             2.4752e-01, -3.6739e-01],\n",
       "           [-3.8251e-01, -2.2150e-01,  2.5250e-01,  ..., -8.7480e-01,\n",
       "            -1.5606e-01, -2.7362e-01],\n",
       "           ...,\n",
       "           [ 3.1199e-01,  1.7777e+00,  1.5525e+00,  ..., -3.3180e-01,\n",
       "             7.2311e-01,  3.7617e-01],\n",
       "           [ 3.6201e-01,  1.4759e-02, -1.7863e-01,  ...,  1.5196e-03,\n",
       "             3.5863e-01, -4.1224e-01],\n",
       "           [-8.3098e-02, -1.7426e-01,  4.7426e-01,  ..., -2.6322e-02,\n",
       "             2.7918e-01, -1.0105e-01]]])))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
